
```{r}
# Library Initiation
library(arrow)
library(tidyverse)
library(writexl)  # used for save data file
library(usmap)
library(ggplot2)
library(maps)
library(ggmap)
library(mapproj)
library(kernlab)
library(caret)

```

```{r}
####### Import the necessary libraries ########

library(arrow)
library(tidyverse)
library(dplyr)
#library(openxlsx)

df_housing <- read_parquet('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/static_house_info.parquet')


########################## DATA PREPARATION ########################## 


################################# WEATHER (COUNTY) ################################# 

unique_counties <- unique(df_housing$in.county)
counties_info <- lapply(unique_counties, function(county) {
  df <- read.csv(paste0('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/weather/2023-weather-data/', county, '.csv'))
  row.names(df) <- NULL
  
  # Convert date_time column from character to POSIXct (datetime)
  df$date_time <- as.POSIXct(df$date_time, format = "%Y-%m-%d %H:%M:%S")

  # Filter data for the month of July
  df <- df[format(df$date_time, "%m") == "07", ]
  df$county <- county
  return(df)
})

Weather_DF <- bind_rows(counties_info)
Weather_DF <- na.omit(Weather_DF)
colnames(Weather_DF) <- c("date_time", "dry_bulb_temp", "relative_humidity", 
                          "wind_speed", "wind_direction", 
                          "global_horz_radiation", "direct_norm_radiation", 
                          "diffuse_horz_radiation","county")

file_path_1 <- "/Users/krutikotadia/Desktop/Fall 2023/Final_Weather_DF.xlsx"
write.xlsx(Weather_DF, file_path_1, rowNames = FALSE)


```


```{r}
# This block of code used to read files from url
# Read files

housefile <- 'https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/static_house_info.parquet'
housedata <- read_parquet(housefile)

energyfile <- 'https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/2023-houseData/433720.parquet'
energydata <- read_parquet(energyfile)


weatherfile <- 'https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/weather/2023-weather-data/G4500010.csv'
weatherdata <- read_csv(weatherfile)

descriptionfile <- 'https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/data_dictionary.csv'
description <- read_csv(descriptionfile)

View(housedata)


Energy_DF <- data.frame()
### Loop over each house
for (i in 1:nrow(df_housing))
{
  building_id <- df_housing[i, "bldg_id"]
  df_building <- read_parquet(paste0('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/2023-houseData/', building_id, '.parquet'))
  df_building$bldg_id <- building_id
  
  ### Convert date_time column and filter July's data
  df_building_july <- df_building[format(as.POSIXct(df_building$time), "%Y-%m") == "2018-07", ]
  Energy_DF <- rbind(Energy_DF,df_building_july)
}


file_path_2 <- "/Users/krutikotadia/Desktop/Fall 2023/Final_Energy_DF.parquet"
write_parquet(Energy_DF, file_path_2)

```

```{r}
# Load necessary libraries
library(foreach)
library(doParallel)

# Set up parallel processing
num_cores <- detectCores() - 1  # Use one less core to leave some resources available
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Initialize an empty dataframe
Energy_DF <- data.frame()

# Loop over each house in parallel
foreach(i = 1:nrow(df_housing), .packages = c("arrow")) %dopar% {
  building_id <- df_housing[i, "bldg_id"]
  df_building <- read_parquet(paste0('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/2023-houseData/', building_id, '.parquet'))
  df_building$bldg_id <- building_id
  
  # Convert date_time column and filter July's data
  df_building_july <- df_building[format(as.POSIXct(df_building$time), "%Y-%m") == "2018-07", ]
  return(df_building_july)
} -> Energy_DF_list  # Store the results in a list

# Stop parallel processing
stopCluster(cl)

# Combine the results into a single dataframe
Energy_DF <- do.call(rbind, Energy_DF_list)

# Write the Energy_DF dataframe to a Parquet file
file_path_2 <- "/Users/krutikotadia/Desktop/Fall 2023/Final_Energy_DF.parquet"
write_parquet(Energy_DF, file_path_2)

```



```{r}
# Data preparation #1
# Merge two data files
# energy and weather

temp_house <- housedata


addressLog <- data.frame(bldg_id = numeric(0),county_id = character(0),energyAddress = character(0),weatherAddress = character(0))

# Output list for peak month
outputList <- data.frame(bldg_id = numeric(0),peak_consumption = numeric(0), peak_month = numeric(0), mean_temperature = numeric(0), mean_humidity = numeric(0),mean_wind_speed = numeric(0))

# Output list for only july
#outputList <- data.frame(bldg_id = numeric(0),energy_consumption = numeric(0), month = numeric(0), mean_temperature = numeric(0), mean_humidity = numeric(0),mean_wind_speed = numeric(0))

# in for loop bracket input specific row to see monthly summary of house that row represents. eg. for (i in 50:50) means showing the info of housing at row 50 in housedata

# output:
# tempdf: merge energy and weather file together
# tempdf_clean: monthly summary of specific building
# addressLog : building id, county id and two data addresses
# outputList: peak energy consumption per month, along with average weather data of that month, labeled with building id can be further merged with housedata for analysis

for (i in 1:nrow(temp_house)){
  # Print the number of iteration being processed
  # For complete iteration input **i in 1:nrow(temp_house)**
  identifier <- paste0("This is ",as.character(i)," iteration")
  print(identifier)
  
  # extract ids from house data
  building_id <- temp_house$bldg_id[i]
  county_id <- temp_house$in.county[i]
  
  # first step create address of energy file and weather file
  energyAddress <- paste0('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/2023-houseData/',building_id,'.parquet')
  weatherAddress <- paste0('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/weather/2023-weather-data/',county_id,'.csv')
 
  addressLog <- add_row(addressLog,bldg_id = building_id, county_id = county_id,energyAddress=energyAddress,weatherAddress=weatherAddress)

  # read energy data and weather data
  tempdf_energy <- read_parquet(energyAddress)
  tempdf_weather <- read_csv(weatherAddress,show_col_types = FALSE)
  
  # merge two data file into one, with time as reference
  tempdf <- merge(tempdf_energy,tempdf_weather,all.x=TRUE,by.x="time",by.y="date_time")
  
  # set time sequence
  # extract time info
  time_str <- tempdf[1] %>%
    mutate(time_as_string = format(time, "%Y-%m-%d %H:%M:%S"),.keep="unused")
  
  # convert time format to string
  tempdf <- data.frame(tempdf,time_str)

  # extract month info, create time sequence and rearrange the dateset
  tempdf$month <- as.numeric(substr(tempdf$time_as_string,6,7))
  
  # clean up the merged file
  tempdf <- tempdf[complete.cases(tempdf$month),]
  
  
  # arrange order of data
  tempdf$time_se <- as.numeric(paste0(substr(tempdf$time_as_string,6,7),substr(tempdf$time_as_string,9,10)))
  tempdf <- arrange(tempdf,tempdf$time_se)
  
  # clean again NA value before next step
  tempdf_clean <- tempdf[complete.cases(tempdf$Dry.Bulb.Temperature...C.),]
  
  # calculate total energy consumption per month
  for (i in 1:nrow(tempdf_clean)){
    tempdf_clean$sum_energy[i] <- sum(tempdf_clean[i,2:43])
  }
  
  # summarise the output
  tempdf_clean <- tempdf_clean %>%
    group_by(month) %>%
    summarise(monthly_usage = sum(sum_energy),monthly_temp = mean(Dry.Bulb.Temperature...C.),monthly_humidity = mean(Relative.Humidity....),monthly_windspeed = mean(Wind.Speed..m.s.))
  
  # output data file
  # Select Peak energy usage
  outputList <- add_row(outputList,bldg_id = building_id,peak_consumption = max(tempdf_clean$monthly_usage), peak_month = which.max(tempdf_clean$monthly_usage), mean_temperature = tempdf_clean$monthly_temp[peak_month], mean_humidity = tempdf_clean$monthly_humidity[peak_month],mean_wind_speed = tempdf_clean$monthly_windspeed[peak_month])
  
  # Select only july data, tempdf_clean[7,]
  #outputList <- add_row(outputList,bldg_id = building_id,energy_consumption = tempdf_clean$monthly_usage[7], month = 7, mean_temperature = tempdf_clean$monthly_temp[month], mean_humidity = tempdf_clean$monthly_humidity[month],mean_wind_speed = tempdf_clean$monthly_windspeed[month])
  
  # remove temporary files
  rm(tempdf_energy)
  rm(tempdf_weather)
  rm(time_str)
  
}
```


```{r}
# This block of code used for merging house data, weather data and energy data together

RawData <- merge(temp_house,outputList,all.x=TRUE,by="bldg_id") 
#peakData <- merge(temp_house,peakdata,all.x=TRUE,by="bldg_id") 

# column 1-51 is house applications and geometries, 52 is total energy consumption in july, 54-56 is average weather info

```


```{r}
# Data preparation #2
# Get 24 hr weather data for specific county
# county id : temperature : humidity : wind speed

temp_county <- housedata

temp_county <- temp_county %>% 
  group_by(in.county) %>% 
  summarise()

outputHr <- NULL

for (i in 1:nrow(temp_county)){
  
  county_id <- temp_county$in.county[i]
  weatherAddress <- paste0('https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/weather/2023-weather-data/',county_id,'.csv')
  tempdf_weather <- read_csv(weatherAddress,show_col_types = FALSE)
  tempdf_weather <- tempdf_weather[4680:4703,1:4]
  tempdf_weather <- data.frame(county_id,tempdf_weather)
  #rm(tempdf_weather)
  outputHr <- rbind(outputHr,tempdf_weather)
}

```


```{r}
# Data preparation #3
# Create a reference list with county id and county name
# The list is used for interactive shiny app

countyName <- c("Abbeville","Aiken","Allendale","Anderson","Bamberg","Barnwell","Beaufort","Berkeley","Calhoun","Charleston","Cherokee","Chester","Chesterfield","Clarendon","Colleton","Darlington","Dillon","Dorchester","Edgefield","Fairfield","Florence","Georgetown","Greenville","Greenwood","Hampton","Horry","Jasper","Kershaw","Lancaster","Laurens","Lee","Lexington","Marion","Marlboro","McCormick","Newberry","Oconee","Orangeburg","Pickens","Richland","Saluda","Spartanburg","Sumter","Union","Williamsburg","York")
county_id <- unique(outputHr$county_id)

county_reference <- data.frame(county_id,countyName)
county_reference$countyName <- tolower(county_reference$countyName)
# merge county_reference with the hourly weather data
regionalData <- merge(county_reference,outputHr,all.x=TRUE,by.x="county_id")

```



```{r}
# This block of code used for saving output list as an excel file

df <- plotFinal
write_xlsx(df, "\\Users\\krutikotadia\\Desktop\\Fall 2023\\IDS 687")

# the path can be anywhere, seperate path with **\\**

```



```{r}
# Linear Modeling
# Use raw data to train the linear regression model used for predicting july's energy condition

# Data filteration, leave only significant variables for modeling
colName <- c(
  "in.sqft",
  "in.clothes_dryer",
  "in.cooling_setpoint",
  "in.geometry_foundation_type",
  "in.geometry_wall_type",
  "in.hvac_has_ducts",
  "in.infiltration",
  "in.occupants",
  "in.insulation_wall",
  "mean_temperature",
  "mean_humidity",
  "mean_wind_speed",
  "energy_consumption"
)
colName

july <- RawData[,colName]



trainList <- createDataPartition(y=july$energy_consumption,p=0.8,list=FALSE)  # Create a list of indices contains 80% of total observations as training data, rest 20% as testing data
trainSet <- july[trainList,]
testSet <- july[-trainList,]

lmJuly <- lm(formula=energy_consumption~.,data=trainSet)
summary(lmJuly)
prediction <- predict(lmJuly,testSet)

ifSuccess <- data.frame(actual=testSet$energy_consumption,pred=prediction)
# identify if the prediction is success
ifSuccess <- ifSuccess %>% 
  mutate(success=between(pred,0.7*actual,1.3*actual))
# if prediction fall within 30% error range then say that's a successful predict
tbl <- table(ifSuccess$success)
cbind(tbl,prop.table(tbl))
# show the accuracy in percentage
## lmJuly is good for prediction

# Export the linear model
#saveRDS(lmJuly, file = "linear_model.rds")
```


```{r}
# Visualization #1
# This block of code used for map visualization attempts
# Draw Map grouped by county
register_stadiamaps("3361de07-653a-4211-ac7c-67613b31c4a7") 

RawData <- rawdata
# position array of houses
selectCol <- c("in.county","in.weather_file_latitude","in.weather_file_longitude","energy_consumption")
countyPos <- RawData[,selectCol]


countyPos <- countyPos %>% 
  group_by(in.county) %>% 
  summarise(aveEnergy=mean(energy_consumption),aveLon=mean(in.weather_file_longitude),aveLat=mean(in.weather_file_latitude))

countyPos <- arrange(countyPos,countyPos$aveEnergy)

us <- c(left = -83.2, bottom = 32.2, right = -78.2, top = 35.7)
map <- get_stadiamap(us, zoom = 7, maptype = "stamen_toner_lite") %>% ggmap()  # US Map

# Scatter plot on base map
mapPlot <- map+geom_point(countyPos,mapping=aes(x=aveLon,y=aveLat,color=aveEnergy,size=aveEnergy))+scale_size(range = c(1,18))+scale_colour_gradient(low="green", high="red")

mapPlot
```


```{r}
# Visualization #2
# Histogram and boxplot visualization attempts of past data

newData <- rawdata
# cooking range
cooking <- newData %>% 
  group_by(in.cooking_range) %>%   # select specific variable as grouping reference
  summarise(Count=n()) %>% 
  mutate(pct = prop.table(Count))

ggplot(cooking, aes(x=in.cooking_range, y=pct,label = scales::percent(pct))) + geom_col() + scale_x_discrete(guide = guide_axis(n.dodge=3)) + labs(title="Cooking Range Appliance")+ geom_text(position = position_dodge(width = .9),vjust = -0.5,size = 3) + scale_y_continuous(labels = scales::percent)+xlab("Cooking Range Types") + ylab("Percentage")

# cloth dryer
# 80,100,120 are three modes for operation, with increasing energy usage
dryer <- newData %>% 
  group_by(in.clothes_dryer) %>%   # select specific variable as grouping reference
  summarise(Count=n()) %>% 
  mutate(pct = prop.table(Count))

ggplot(dryer, aes(x=in.clothes_dryer, y=pct,label = scales::percent(pct))) + geom_col() + scale_x_discrete(guide = guide_axis(n.dodge=3)) + labs(title="Clothes Dryer Appliance")+ geom_text(position = position_dodge(width = .9),vjust = -0.5,size = 3) + scale_y_continuous(labels = scales::percent)+xlab("Clothes Dryer Types") + ylab("Percentage")

# in.cooling_setpoint
# 78F is sweet spot for AC to balance energy saving and comfort
cooling <- newData %>% 
  group_by(in.cooling_setpoint) %>%   # select specific variable as grouping reference
  summarise(Count=n()) %>% 
  mutate(pct = prop.table(Count))

ggplot(cooling, aes(x=in.cooling_setpoint, y=pct,label = scales::percent(pct))) + geom_col() + scale_x_discrete(guide = guide_axis(n.dodge=3)) + labs(title="Clothes Dryer Appliance")+ geom_text(position = position_dodge(width = .9),vjust = -0.5,size = 3) + scale_y_continuous(labels = scales::percent)+xlab("Cooling Setpoint") + ylab("Percentage")

# 


ggplot(data=newData) + aes(x=energy_consumption, y=in.hvac_has_ducts) + geom_point() + geom_boxplot()

```



